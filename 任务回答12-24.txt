
A: cache就是最后调用persist(strorage.memory_only）
===============================================
Q: 那个UI有两个tab看不了是怎么回事？
UI有两个tab看不了 这个问题还没有弄清楚，
因为在Linux集群中可以配置历史任务记录服务器，任务运行完任务可以回看，
windows上不行，但是Linux服务器提供的是内网Ip 在浏览器输入地址打不开。
最近主要在看spark内存管理和任务调度全链路
===============================================
Q: 咱们跑benchmark, 有什么性能指标可以看persist了多少，cache了多少？
persist和cache的操作都是由spark内部完成的，正在深入。
目前知道的是 如果以磁盘方式保存会保存在 本地系统 的临时文件中，运行完这个任务便会自动清除，
也就是说去可以监控临时文件写入大小
如果保存在内存中，会将rdd对象保存在storage空间（可以理解为spark将空间大致分为storage空间-- 用于存放rdd缓存 和 executor空间 -- 执行过程需要的内存），
当executor空间不够多时会向storage借用空间，当storage剩余可用空间不够多时
会将保存在storage空间中的rdd(之前通过cache存储的rdd)清除一部分，释放空间给executor空间用。

举例：
假设分为给:
executor空间--500M 
storage空间--500M
-------------------------------
使用一段时间
执行了一些操作：
- ...
- rdd1.cache();
- rdd2.cache();
--------------------------------
executor可用空间100M
storage可用空间100M
stoarge空间存放了两个rdd
rdd1(200M)--rdd2(200M)
--------------------------------
当有一个任务需要80M任务，那么直接从executor可用空间（100M）取80M即可；
当有一个任务需要200M任务，executor可用空间（100M-80M=20M）只有20M,不够用，还需要180M,
那么会向stoarge空间借用，发现stoarge可用空间只有100M,不够用，
这时候会淘汰掉rdd1(200M)---会存储到磁盘中，内存中对应部分删除,
这时候storage可用空间（100+200=300）300M,便可以借给executor空间200M

也就是说存放在storage可用空间有可能会保存到磁盘的临时文件中去（内存紧张时）
===============================================
Q: persist  也写到HDFS上吗？还是只有checkpoint写到HDFS上？
persist 写到本地系统临时文件，任务运行完便会删除。
checkpoint一般是写到hdfs中，但是在本地模式下，可以选择写到本地文件系统，任务运行完不会删除。
其他模式下（集群模式和yarn模式）不可以写到本地文件系统，
会出现逻辑错误（文件写到A主机的本地文件系统，任务到B主机的文件系统找，发现找不到）

===============================================
Q: benchmark一开始运行，spark都会做些什么操作，读写什么文件，
   为什么这么做，iostat会是多少，其它spark 的性能指标都有多少?
driver: 任务调度者
executor:实际任务执行者
整个运行流程会涉及：
- 阶段的划分
- 任务的提交
- 资源的准备
- 节点间的通信（本地模式下依然需要通信，driver将任务下发到各个executor,各个executor向driver汇报完成情况）
- 最后将结果收集到driver

详细流程还在梳理，整个调用链路太长，涉及内存管理，任务调度等...
具体指标还需要研究
===============================================
Q: spark有没有granfan对接的这样的例子？
Prometheus应该可以监控 spark任务运行情况





